{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset that contains images' path and label\n",
    "data = pd.DataFrame()\n",
    "data['image-name'] = os.listdir('../data/flowers/')\n",
    "\n",
    "# extracting species\n",
    "data['species'] = data['image-name'].apply(lambda x: int(x[:2]))\n",
    "\n",
    "# saving \"true\" label\n",
    "np.save('../data/true_label.npy', data['species'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### raw images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_images_flatten = np.array([plt.imread('../data/flowers/'+img).ravel() for img in data['image-name'].tolist()])\n",
    "# saving raw images\n",
    "np.save('../data/raw_images_flatten.npy', raw_images_flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### applying pca to reduce images' dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dim(imgs, k):\n",
    "    pca = PCA(n_components=k)\n",
    "    return pca.fit_transform(imgs)\n",
    "\n",
    "images_600 = reduce_dim(raw_images_flatten, 600)\n",
    "# saving reduced images\n",
    "np.save('../data/images_600.npy', images_600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using a neural network to create a better mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, concatenate,  Dropout, Conv2D, MaxPool2D\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# embeddings size\n",
    "EMBED_SIZE = 128\n",
    "\n",
    "input_image = Input(shape=(128, 128, 4))\n",
    "x = Conv2D(32, (3, 3), activation='relu')(input_image)\n",
    "x = MaxPool2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = MaxPool2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu')(x)\n",
    "x = MaxPool2D((2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(EMBED_SIZE, activation='relu')(x)\n",
    "\n",
    "base_network = Model(inputs=input_image, outputs=x)\n",
    "\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    anchor, positive, negative = y_pred[:, :EMBED_SIZE], y_pred[:, EMBED_SIZE:2*EMBED_SIZE], y_pred[:, 2*EMBED_SIZE:]\n",
    "    positive_dist = tf.reduce_mean(tf.square(anchor - positive), axis=1)\n",
    "    negative_dist = tf.reduce_mean(tf.square(anchor - negative), axis=1)\n",
    "    return tf.maximum(positive_dist - negative_dist + 0.2, 0.)\n",
    "\n",
    "input_image_one = Input(shape=(128, 128, 4), name='input_image_one') # input layer for image one\n",
    "input_image_two = Input(shape=(128, 128, 4), name='input_image_two') # input layer for image two\n",
    "input_image_three = Input(shape=(128, 128, 4), name='input_image_three') # input layer for image three\n",
    "\n",
    "out = concatenate([base_network(input_image_one), base_network(input_image_two),\n",
    "                             base_network(input_image_three)])\n",
    "\n",
    "model = Model(inputs=[input_image_one, input_image_two, input_image_three],\n",
    "            outputs=out)\n",
    "model.compile(loss=triplet_loss, optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_batch_generator(images_names_df, embed_size, batchsize=32):\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        images_names = images_names_df.sample(batchsize)['image-name'].tolist()\n",
    "        anchors, positives, negatives, masks = [], [], [], []\n",
    "        \n",
    "        for img_name in images_names:\n",
    "            \n",
    "            anchor = plt.imread('../data/flowers/'+img_name)\n",
    "            anchor_class = int(img_name[:2])\n",
    "            \n",
    "            positive_example = images_names_df[images_names_df['species'] == anchor_class].sample(1).iloc[0]['image-name']\n",
    "            negative_example = images_names_df[images_names_df['species'] != anchor_class].sample(1).iloc[0]['image-name']\n",
    "            \n",
    "            anchors.append(anchor)\n",
    "            positives.append(plt.imread('../data/flowers/'+positive_example))\n",
    "            negatives.append(plt.imread('../data/flowers/'+negative_example))\n",
    "            masks.append(np.zeros(3 * embed_size))\n",
    "            \n",
    "        yield [np.array(anchors), np.array(positives), np.array(negatives)], np.array(masks)\n",
    "#         instances, masks = [], []    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caio/Documents/am1/am1/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "60/60 [==============================] - 23s 369ms/step - loss: 0.1463\n",
      "Epoch 2/15\n",
      "60/60 [==============================] - 22s 367ms/step - loss: 0.1061\n",
      "Epoch 3/15\n",
      "60/60 [==============================] - 22s 367ms/step - loss: 0.0914\n",
      "Epoch 4/15\n",
      "60/60 [==============================] - 22s 370ms/step - loss: 0.0770\n",
      "Epoch 5/15\n",
      "60/60 [==============================] - 22s 366ms/step - loss: 0.0602\n",
      "Epoch 6/15\n",
      "60/60 [==============================] - 22s 364ms/step - loss: 0.0539\n",
      "Epoch 7/15\n",
      "60/60 [==============================] - 22s 366ms/step - loss: 0.0379\n",
      "Epoch 8/15\n",
      "60/60 [==============================] - 22s 365ms/step - loss: 0.0359\n",
      "Epoch 9/15\n",
      "60/60 [==============================] - 22s 365ms/step - loss: 0.0362\n",
      "Epoch 10/15\n",
      "60/60 [==============================] - 22s 365ms/step - loss: 0.0253\n",
      "Epoch 11/15\n",
      "60/60 [==============================] - 22s 365ms/step - loss: 0.0258\n",
      "Epoch 12/15\n",
      "60/60 [==============================] - 22s 366ms/step - loss: 0.0214\n",
      "Epoch 13/15\n",
      "60/60 [==============================] - 22s 366ms/step - loss: 0.0249\n",
      "Epoch 14/15\n",
      "60/60 [==============================] - 22s 367ms/step - loss: 0.0223\n",
      "Epoch 15/15\n",
      "60/60 [==============================] - 22s 368ms/step - loss: 0.0194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f46ba828d00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen = image_batch_generator(data, EMBED_SIZE)\n",
    "model.fit_generator(train_gen, epochs=15, steps_per_epoch=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_images = np.array([plt.imread('../data/flowers/'+img) for img in data['image-name'].tolist()])\n",
    "# creating embeddings for each image\n",
    "embeddings = base_network.predict(raw_images)\n",
    "# saving embeddings' images\n",
    "np.save('../data/full_data_embeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using 10% of images - simulating a manual labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_image_one, input_image_two, input_image_three],\n",
    "            outputs=out)\n",
    "model.compile(loss=triplet_loss, optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caio/Documents/am1/am1/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.0119\n",
      "Epoch 2/15\n",
      "10/10 [==============================] - 4s 363ms/step - loss: 0.0063\n",
      "Epoch 3/15\n",
      "10/10 [==============================] - 4s 362ms/step - loss: 6.4026e-04\n",
      "Epoch 4/15\n",
      "10/10 [==============================] - 4s 363ms/step - loss: 0.0029\n",
      "Epoch 5/15\n",
      "10/10 [==============================] - 4s 361ms/step - loss: 0.0022\n",
      "Epoch 6/15\n",
      "10/10 [==============================] - 4s 364ms/step - loss: 0.0000e+00\n",
      "Epoch 7/15\n",
      "10/10 [==============================] - 4s 364ms/step - loss: 9.1688e-05\n",
      "Epoch 8/15\n",
      "10/10 [==============================] - 4s 360ms/step - loss: 0.0047\n",
      "Epoch 9/15\n",
      "10/10 [==============================] - 4s 360ms/step - loss: 0.0030\n",
      "Epoch 10/15\n",
      "10/10 [==============================] - 4s 361ms/step - loss: 0.0000e+00\n",
      "Epoch 11/15\n",
      "10/10 [==============================] - 4s 361ms/step - loss: 0.0000e+00\n",
      "Epoch 12/15\n",
      "10/10 [==============================] - 4s 360ms/step - loss: 0.0000e+00\n",
      "Epoch 13/15\n",
      "10/10 [==============================] - 4s 365ms/step - loss: 0.0000e+00\n",
      "Epoch 14/15\n",
      "10/10 [==============================] - 4s 364ms/step - loss: 4.2981e-04\n",
      "Epoch 15/15\n",
      "10/10 [==============================] - 4s 369ms/step - loss: 0.0018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f46bf8d9040>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_undersized = data.sample(60)\n",
    "train_gen = image_batch_generator(data_undersized, EMBED_SIZE, batchsize=16)\n",
    "model.fit_generator(train_gen, epochs=15, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_images = np.array([plt.imread('../data/flowers/'+img) for img in data['image-name'].tolist()])\n",
    "# creating embeddings for each image\n",
    "embeddings = base_network.predict(raw_images)\n",
    "# saving embeddings' images\n",
    "np.save('../data/partial_data_embeddings.npy', embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
